{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hijuli66/33/blob/master/Streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok sentence-transformers keybert -q\n",
        "!pip install -q torchxrayvision"
      ],
      "metadata": {
        "id": "yi834X1BcaBS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fdfb1d8-04ff-40df-a96f-7882b109c87f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "!ngrok authtoken 36xpcdxUNo8UH0GzDh6G7E3pugM_7PNih33G5fa7d9A9hXKMF\n",
        "\n",
        "!streamlit run app.py &>/dev/null&\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"è®¿é—®åœ°å€ï¼š\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwD5QGefc-1v",
        "outputId": "f62244e7-571e-4b13-b5cb-3d3b61a3786d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "è®¿é—®åœ°å€ï¼š NgrokTunnel: \"https://carditic-stephine-nonaccredited.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å…ˆæŒ‚è½½ Google Driveï¼ˆå¦‚æœè¿˜æ²¡æŒ‚è½½çš„è¯ï¼‰\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_18Vm6kheKNy",
        "outputId": "cde99893-62b1-4a61-c609-7ba08ac5c6ee"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# æ€æ‰æ‰€æœ‰ç°æœ‰çš„ ngrok éš§é“\n",
        "ngrok.kill()\n",
        "\n",
        "# é‡æ–°å¯åŠ¨ Streamlitï¼ˆå¦‚æœè¢«æ€äº†çš„è¯ï¼‰\n",
        "!pkill -f streamlit\n",
        "!streamlit run app.py --server.port=8501 --server.address=0.0.0.0 &>/dev/null &\n",
        "\n",
        "# ç­‰å¾…å¯åŠ¨\n",
        "import time\n",
        "time.sleep(10)\n",
        "\n",
        "# åªåˆ›å»ºä¸€ä¸ªæ–°éš§é“\n",
        "public_url = ngrok.connect(8501, bind_tls=True)\n",
        "print(\"æ–°çš„å…¬ç½‘è®¿é—®åœ°å€ï¼š\", public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JZqGmhEgxGC",
        "outputId": "6233635d-5d36-4c56-8407-31754db2d4ee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ–°çš„å…¬ç½‘è®¿é—®åœ°å€ï¼š NgrokTunnel: \"https://carditic-stephine-nonaccredited.ngrok-free.dev\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# ==================== app.py ====================\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import os\n",
        "import re\n",
        "import torchxrayvision as xrv\n",
        "import torchvision.transforms as transforms   # å¦‚æœè¿˜æ²¡å¯¼å…¥çš„è¯ï¼ŒåŠ ä¸Šè¿™è¡Œ\n",
        "\n",
        "# ------------------- é…ç½® -------------------\n",
        "st.set_page_config(page_title=\"èƒ¸éƒ¨Xå…‰AIè¾…åŠ©è¯Šæ–­ç³»ç»Ÿ\", layout=\"wide\")\n",
        "st.title(\"èƒ¸éƒ¨Xå…‰è‚ºç‚AIè¾…åŠ©è¯Šæ–­ç³»ç»Ÿ\")\n",
        "st.markdown(\"\"\"\n",
        "### è¯·ä¸Šä¼ èƒ¸éƒ¨Xå…‰å›¾åƒå’Œå¯¹åº”çš„æ”¾å°„ç§‘æŠ¥å‘Š\n",
        "ç³»ç»Ÿå°†å®æ—¶æä¾›ï¼š\n",
        "- Grad-CAM çƒ­åŠ›å›¾ï¼ˆé«˜äº®è‚ºç‚å¯ç–‘åŒºåŸŸï¼‰\n",
        "- æŠ¥å‘Šå…³é”®è¯è‡ªåŠ¨æå–ä¸é«˜äº®\n",
        "- å›¾æ–‡åŒ¹é…ç›¸ä¼¼åº¦ + AI è¯Šæ–­å»ºè®®\n",
        "\"\"\")\n",
        "\n",
        "# ------------------- é¡¹ç›®è·¯å¾„ -------------------\n",
        "PROJECT_PATH = '/content/drive/MyDrive/33'\n",
        "MOBILENET_PATH = f'{PROJECT_PATH}/Images/Models/mobilenetv3_final.pth'\n",
        "CONTRASTIVE_PATH = f'{PROJECT_PATH}/Matching/best_contrastive_model.pth'\n",
        "\n",
        "\n",
        "@st.cache_resource\n",
        "def load_models():\n",
        "    st.info(\"æ­£åœ¨åŠ è½½æ‰€æœ‰æ¨¡å‹ï¼ˆMobileNetV3 + å›¾æ–‡åŒ¹é… + è‚ºéƒ¨åˆ†å‰²ï¼‰ï¼Œé¦–æ¬¡è¿è¡Œéœ€60-90ç§’...\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # ==================== 1. MobileNetV3 åˆ†ç±»æ¨¡å‹ ====================\n",
        "    mobilenet = models.mobilenet_v3_small(pretrained=False)\n",
        "    mobilenet.features[0][0] = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "    num_features = mobilenet.classifier[3].in_features\n",
        "    mobilenet.classifier[3] = nn.Linear(num_features, 2)\n",
        "\n",
        "    state_dict = torch.load(MOBILENET_PATH, map_location=device)\n",
        "    model_dict = mobilenet.state_dict()\n",
        "    filtered_dict = {k: v for k, v in state_dict.items() if k in model_dict and v.shape == model_dict[k].shape}\n",
        "    mobilenet.load_state_dict(filtered_dict, strict=False)\n",
        "    mobilenet.to(device)\n",
        "    mobilenet.eval()\n",
        "    final_conv_layer = mobilenet.features[-1]\n",
        "\n",
        "    # ==================== 2. å›¾æ–‡å¯¹æ¯”æ¨¡å‹ ====================\n",
        "    class ContrastiveModel(nn.Module):\n",
        "        def __init__(self, img_dim=576, text_dim=768, embed_dim=256):\n",
        "            super().__init__()\n",
        "            self.proj_img = nn.Linear(img_dim, embed_dim)\n",
        "            self.proj_text = nn.Linear(text_dim, embed_dim)\n",
        "\n",
        "        def forward(self, img=None, text=None):\n",
        "            if img is not None:\n",
        "                return F.normalize(self.proj_img(img), dim=-1)\n",
        "            if text is not None:\n",
        "                return F.normalize(self.proj_text(text), dim=-1)\n",
        "\n",
        "    contrastive_model = ContrastiveModel()\n",
        "    contrastive_model.load_state_dict(torch.load(CONTRASTIVE_PATH, map_location=device))\n",
        "    contrastive_model.to(device)\n",
        "    contrastive_model.eval()\n",
        "\n",
        "    # ==================== 3. æ–‡æœ¬ç¼–ç å™¨ ====================\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    embedder = SentenceTransformer('shibing624/text2vec-base-chinese')\n",
        "    embedder.to(device)\n",
        "\n",
        "    # ==================== 4. è‚ºéƒ¨åˆ†å‰²æ¨¡å‹ ====================\n",
        "    lung_seg_model = xrv.baseline_models.chestx_det.PSPNet()\n",
        "    lung_seg_model.to(device)\n",
        "    lung_seg_model.eval()\n",
        "\n",
        "    # åˆ é™¤æœ‰é—®é¢˜çš„transformå®šä¹‰ï¼Œæ›¿æ¢ä¸ºï¼š\n",
        "    lung_transform = None  # ä¸ä½¿ç”¨transformï¼Œç›´æ¥åœ¨å‡½æ•°ä¸­å¤„ç†\n",
        "\n",
        "\n",
        "    st.success(\"æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆï¼\")\n",
        "\n",
        "    return (device, mobilenet, final_conv_layer, contrastive_model,\n",
        "            embedder, lung_seg_model, lung_transform)\n",
        "\n",
        "# ------------------- å…³é”®è¯æå–å‡½æ•° -------------------\n",
        "def extract_keywords(text, top_n=20):\n",
        "    if not text or not isinstance(text, str):\n",
        "        return []\n",
        "    text = text.strip()\n",
        "    if len(text) == 0:\n",
        "        return []\n",
        "\n",
        "    keywords = set()\n",
        "    text_clean = text.replace(' ', '')\n",
        "\n",
        "    basic_patterns = [\n",
        "        r'ç£¨ç»ç’ƒå½±',r'è‚ºç‚',\n",
        "        r'åŒè‚º', r'è‚ºé‡', r'çº¹ç†', r'æ–‘ç‰‡å½±', r'æ¡ç´¢å½±',\n",
        "        r'å‘çƒ­', r'å’³å—½', r'å¹²å’³', r'å’³ç—°'\n",
        "    ]\n",
        "    for p in basic_patterns:\n",
        "        keywords.update(re.findall(p, text_clean))\n",
        "\n",
        "    symptoms = re.findall(r'(ä¹åŠ›|è‚Œè‚‰é…¸ç—›|å’½ç—›|èƒ¸é—·|å‘¼å¸å›°éš¾|æ°”ä¿ƒ|å¤´ç—›|æ¶å¿ƒ|å‘•å|è…¹æ³»)', text_clean)\n",
        "    keywords.update(symptoms)\n",
        "\n",
        "    epi_history = re.findall(r'(ç–«åŒºæ—…å±…å²|æ–°å† æ‚£è€…æ¥è§¦å²|å¯†åˆ‡æ¥è§¦å²)', text_clean)\n",
        "    keywords.update(epi_history)\n",
        "\n",
        "    vaccine = re.findall(r'æ¥ç§æ–°å† ç–«è‹—\\s*\\d+å‰‚', text_clean)\n",
        "    keywords.update(vaccine)\n",
        "\n",
        "    nucleic = re.findall(r'(æ–°å† ç—…æ¯’æ ¸é…¸|æ ¸é…¸)\\s*(é˜³æ€§|é˜´æ€§)', text_clean)\n",
        "    antibody = re.findall(r'(æ–°å† ç—…æ¯’æŠ—ä½“|æŠ—ä½“)\\s*(é˜³æ€§|é˜´æ€§)', text_clean)\n",
        "    for a, b in (nucleic + antibody):\n",
        "        keywords.add(f\"{a}{b}\")\n",
        "\n",
        "    img_short_patterns = [\n",
        "        r'åŒè‚ºå¤šå‘ç£¨ç»ç’ƒå½±', r'[å·¦å³][ä¸Šä¸‹]è‚º[^ã€‚ï¼›,]*?å®å˜', r'è‚ºé‡æ¸…æ™°', r'æ— æ˜æ˜¾å¼‚å¸¸',\n",
        "        r'åŒè‚ºçº¹ç†å¢å¤š', r'æ–‘ç‰‡å½±', r'æ¡ç´¢å½±', r'å¿ƒå½±æ­£å¸¸', r'è‚ºé—¨å½±æœªè§å¢å¤§',\n",
        "        r'è‚ºéƒ¨é€šæ°”è‰¯å¥½', r'æ— ç‚ç—‡å½±'\n",
        "    ]\n",
        "    for p in img_short_patterns:\n",
        "        keywords.update(re.findall(p, text_clean))\n",
        "\n",
        "    ct_long_patterns = [\n",
        "        r'èƒ¸éƒ¨CT[^ã€‚ï¼›,]*?åŒè‚º[^ã€‚ï¼›,]*?ç£¨ç»ç’ƒå½±[^ã€‚ï¼›,]*?',\n",
        "        r'[å·¦å³][ä¸Šä¸‹]è‚º[^ã€‚ï¼›,]*?ç‚ç—‡æ€§æ”¹å˜[^ã€‚ï¼›,]*?',\n",
        "        r'å³è‚ºä¸‹å¶å®å˜[^ã€‚ï¼›,]*?',\n",
        "        r'è‚ºé‡æ¸…æ™°[^ã€‚ï¼›,]*?å¿ƒå½±æ­£å¸¸[^ã€‚ï¼›,]*?æ— å¼‚å¸¸å¯†åº¦å½±',\n",
        "        r'Xå…‰èƒ¸ç‰‡[^ã€‚ï¼›,]*?åŒè‚ºçº¹ç†æ­£å¸¸[^ã€‚ï¼›,]*?è‚ºé—¨å½±æœªè§å¢å¤§',\n",
        "        r'è‚ºéƒ¨é€šæ°”è‰¯å¥½[^ã€‚ï¼›,]*?æ— ç‚ç—‡å½±'\n",
        "    ]\n",
        "    for p in ct_long_patterns:\n",
        "        keywords.update(re.findall(p, text_clean))\n",
        "\n",
        "    diag_patterns = [\n",
        "        r'æ–°å‹å† çŠ¶ç—…æ¯’è‚ºç‚', r'ç—…æ¯’æ€§è‚ºç‚', r'ç¤¾åŒºè·å¾—æ€§è‚ºç‚', r'æ–°å† è‚ºç‚',\n",
        "        r'è½»å‹|æ™®é€šå‹|é‡å‹|å±é‡å‹'\n",
        "    ]\n",
        "    for p in diag_patterns:\n",
        "        keywords.update(re.findall(p, text_clean))\n",
        "\n",
        "    result = list(keywords)\n",
        "    result.sort(key=len, reverse=True)\n",
        "    return result[:top_n]\n",
        "\n",
        "# ------------------- Grad-CAM å‡½æ•° -------------------\n",
        "def generate_gradcam(image_pil, model, target_layer):\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "    ])\n",
        "\n",
        "    input_tensor = preprocess(image_pil).unsqueeze(0).to(device)\n",
        "\n",
        "    original_mode = model.training\n",
        "    model.train()\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    activations = None\n",
        "    gradients = None\n",
        "\n",
        "    def forward_hook(module, input, output):\n",
        "        nonlocal activations\n",
        "        activations = output\n",
        "        activations.retain_grad()\n",
        "\n",
        "    def backward_hook(module, grad_input, grad_output):\n",
        "        nonlocal gradients\n",
        "        gradients = grad_output[0]\n",
        "\n",
        "    forward_handle = target_layer.register_forward_hook(forward_hook)\n",
        "    backward_handle = target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    try:\n",
        "        model.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(True):\n",
        "            output = model(input_tensor)\n",
        "            pred_class = output.argmax(dim=1).item()\n",
        "            score = output[0, pred_class]\n",
        "            score.backward()\n",
        "\n",
        "        if gradients is None:\n",
        "            print(\"è­¦å‘Š: æœªé€šè¿‡hookè·å–æ¢¯åº¦ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•...\")\n",
        "            model.zero_grad()\n",
        "            one_hot = torch.zeros_like(output)\n",
        "            one_hot[0, pred_class] = 1.0\n",
        "            with torch.set_grad_enabled(True):\n",
        "                output = model(input_tensor)\n",
        "                output.backward(gradient=one_hot)\n",
        "            if activations.grad is not None:\n",
        "                gradients = activations.grad\n",
        "\n",
        "        grads_val = gradients.detach().cpu().numpy().squeeze()\n",
        "        acts_val = activations.detach().cpu().numpy().squeeze()\n",
        "        weights = np.mean(grads_val, axis=(1, 2))\n",
        "\n",
        "        cam = np.zeros(acts_val.shape[1:], dtype=np.float32)\n",
        "        for i, w in enumerate(weights):\n",
        "            cam += w * acts_val[i]\n",
        "\n",
        "        cam = np.maximum(cam, 0)\n",
        "        cam = cv2.resize(cam, (image_pil.width, image_pil.height))\n",
        "        cam = cam - np.min(cam)\n",
        "        cam = cam / (np.max(cam) + 1e-8)\n",
        "\n",
        "        print(f\"Grad-CAMç”Ÿæˆå®Œæˆï¼Œå°ºå¯¸: {cam.shape}\")\n",
        "\n",
        "                # ===== åº”ç”¨è‚ºéƒ¨æ©ç ï¼ˆæ­£ç¡®ä½ç½®å’Œç¼©è¿›ï¼‰=====\n",
        "        try:\n",
        "            lung_mask = generate_lung_mask(image_pil)  # è¿”å› (H, W) float32 numpy array\n",
        "\n",
        "            # å¦‚æœå°ºå¯¸ä¸åŒ¹é…ï¼Œè°ƒæ•´æ©ç å¤§å°\n",
        "            if lung_mask.shape != cam.shape:\n",
        "                lung_mask = cv2.resize(lung_mask, (cam.shape[1], cam.shape[0]),\n",
        "                                       interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            # è½»å¾®è†¨èƒ€æ©ç ï¼Œä½¿è¾¹ç•Œè¿‡æ¸¡æ›´è‡ªç„¶\n",
        "            kernel = np.ones((25, 25), np.uint8)\n",
        "            lung_mask_dilated = cv2.dilate(lung_mask, kernel, iterations=1)\n",
        "\n",
        "            # è½¯èåˆï¼šä¸»è¦ç”¨åŸæ©ç  + è½»å¾®æ‰©å±•\n",
        "            combined_mask = np.maximum(lung_mask, lung_mask_dilated * 0.4)\n",
        "\n",
        "            # åº”ç”¨åˆ°çƒ­åŠ›å›¾\n",
        "            cam = cam * combined_mask\n",
        "\n",
        "            # é‡æ–°å½’ä¸€åŒ–\n",
        "            cam = np.maximum(cam, 0)\n",
        "            if cam.max() > 0:\n",
        "                cam = cam / cam.max()\n",
        "\n",
        "            st.caption(\"âœ” å·²æˆåŠŸåº”ç”¨è‚ºéƒ¨åˆ†å‰²æ©ç ï¼Œçƒ­åŠ›å›¾ä»…åœ¨è‚ºéƒ¨åŒºåŸŸé«˜äº®\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.warning(f\"è‚ºéƒ¨æ©ç åº”ç”¨å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹çƒ­åŠ›å›¾: {e}\")\n",
        "\n",
        "    finally:\n",
        "        forward_handle.remove()\n",
        "        backward_handle.remove()\n",
        "        model.train(original_mode)\n",
        "\n",
        "    return cam\n",
        "def generate_lung_mask(image_pil):\n",
        "    \"\"\"\n",
        "    é«˜è´¨é‡è‚ºéƒ¨æ©ç ç”Ÿæˆå‡½æ•°ï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # å°†PILå›¾åƒè½¬æ¢ä¸ºnumpyæ•°ç»„\n",
        "        img_np = np.array(image_pil)\n",
        "\n",
        "        # å¦‚æœå›¾åƒæ˜¯å½©è‰²ï¼Œè½¬æ¢ä¸ºç°åº¦\n",
        "        if len(img_np.shape) == 3:\n",
        "            img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        # ç¡®ä¿æ˜¯uint8ç±»å‹ï¼ˆ0-255ï¼‰\n",
        "        if img_np.dtype != np.uint8:\n",
        "            img_np = (img_np * 255).astype(np.uint8) if img_np.max() <= 1.0 else img_np.astype(np.uint8)\n",
        "\n",
        "        # ä¸­å¿ƒè£å‰ªæˆæ­£æ–¹å½¢ï¼ˆä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„é€»è¾‘ï¼‰\n",
        "        def center_crop_to_square(img_np):\n",
        "            h, w = img_np.shape[:2]\n",
        "            min_side = min(h, w)\n",
        "            start_h = (h - min_side) // 2\n",
        "            start_w = (w - min_side) // 2\n",
        "            return img_np[start_h:start_h+min_side, start_w:start_w+min_side]\n",
        "\n",
        "        img_square = center_crop_to_square(img_np)\n",
        "\n",
        "        # **ä¿®å¤ï¼šæ­£ç¡®çš„æ•°æ®é¢„å¤„ç†æ–¹å¼**\n",
        "        # 1. å½’ä¸€åŒ–åˆ°[0,1]ï¼ˆä½¿ç”¨xrv.datasets.normalizeï¼‰\n",
        "        img_normalized = xrv.datasets.normalize(img_square, 255)\n",
        "\n",
        "        # 2. è½¬æ¢ä¸ºtorchå¼ é‡\n",
        "        img_tensor = torch.from_numpy(img_normalized).float().unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        # 3. è°ƒæ•´å¤§å°åˆ°512x512\n",
        "        img_tensor = torch.nn.functional.interpolate(\n",
        "            img_tensor,\n",
        "            size=(512, 512),\n",
        "            mode='bilinear',\n",
        "            align_corners=False\n",
        "        ).to(device)\n",
        "\n",
        "        # 4. æ¨ç†è·å–åˆ†å‰²ç»“æœ\n",
        "        with torch.no_grad():\n",
        "            output = lung_seg_model(img_tensor)\n",
        "            prob = torch.softmax(output, dim=1)\n",
        "\n",
        "        # 5. æå–å·¦å³è‚ºæ¦‚ç‡ï¼ˆç±»åˆ«4å’Œ5ï¼‰\n",
        "        lung_prob = torch.max(prob[0, [4, 5]], dim=0)[0].cpu().numpy()\n",
        "\n",
        "        # 6. é˜ˆå€¼åŒ–è·å–æ©ç \n",
        "        mask_512 = (lung_prob > 0.4).astype(np.uint8)\n",
        "\n",
        "        # 7. å½¢æ€å­¦æ“ä½œæ”¹å–„æ©ç \n",
        "        mask_512 = cv2.morphologyEx(mask_512, cv2.MORPH_CLOSE, np.ones((7,7), np.uint8), iterations=3)\n",
        "        mask_512 = cv2.morphologyEx(mask_512, cv2.MORPH_OPEN, np.ones((5,5), np.uint8), iterations=1)\n",
        "\n",
        "        # 8. è°ƒæ•´æ©ç åˆ°åŸå§‹å›¾åƒå°ºå¯¸\n",
        "        mask_resized = cv2.resize(\n",
        "            mask_512,\n",
        "            (img_np.shape[1], img_np.shape[0]),\n",
        "            interpolation=cv2.INTER_NEAREST\n",
        "        )\n",
        "\n",
        "        # 9. è½¬æ¢ä¸ºfloat32ç±»å‹ï¼ˆç”¨äºåç»­ä¹˜æ³•æ“ä½œï¼‰\n",
        "        mask_resized = mask_resized.astype(np.float32)\n",
        "\n",
        "        return mask_resized\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"è‚ºéƒ¨æ©ç ç”Ÿæˆå¤±è´¥: {str(e)}\")\n",
        "        # è¿”å›å…¨1æ©ç ä½œä¸ºå¤‡é€‰\n",
        "        return np.ones_like(np.array(image_pil).astype(np.float32))\n",
        "\n",
        "\n",
        "# ------------------- ç”¨æˆ·ä¸Šä¼ åŠåç»­é€»è¾‘ -------------------\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "    uploaded_image = st.file_uploader(\"ä¸Šä¼ èƒ¸éƒ¨Xå…‰å›¾åƒ\", type=['jpg', 'png', 'jpeg'])\n",
        "with col2:\n",
        "    uploaded_report_file = st.file_uploader(\"ä¸Šä¼ æ”¾å°„ç§‘æŠ¥å‘Šï¼ˆ.txtï¼‰\", type=['txt'])\n",
        "    report_text = st.text_area(\"æˆ–è€…ç›´æ¥åœ¨æ­¤ç²˜è´´æŠ¥å‘Šæ–‡æœ¬\", height=200)\n",
        "\n",
        "if uploaded_report_file is not None:\n",
        "    report_text = uploaded_report_file.read().decode(\"utf-8\")\n",
        "\n",
        "if uploaded_image is not None and report_text.strip() != \"\":\n",
        "    # Xå…‰å›¾è½¬ä¸ºç°åº¦ï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰\n",
        "    image_pil = Image.open(uploaded_image).convert('L')\n",
        "    device, mobilenet, final_conv_layer, contrastive_model, embedder, lung_seg_model, lung_transform = load_models()\n",
        "\n",
        "    with st.spinner(\"æ­£åœ¨åˆ†æå›¾åƒä¸æŠ¥å‘Šï¼Œè¯·ç¨å€™...\"):\n",
        "        # 1. å…ˆè¿›è¡Œ Grad-CAMï¼ˆéœ€è¦æ¢¯åº¦ï¼‰\n",
        "        try:\n",
        "            cam = generate_gradcam(image_pil, mobilenet, final_conv_layer)\n",
        "            cam_color = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
        "            cam_color = cv2.cvtColor(cam_color, cv2.COLOR_BGR2RGB)\n",
        "            overlay = cv2.addWeighted(np.array(image_pil.convert('RGB')), 0.65, cam_color, 0.35, 0)\n",
        "            overlay_pil = Image.fromarray(overlay)\n",
        "\n",
        "            cam_success = True\n",
        "        except Exception as e:\n",
        "            st.error(f\"Grad-CAMç”Ÿæˆå¤±è´¥: {e}\")\n",
        "            overlay_pil = image_pil.convert('RGB')\n",
        "            cam_success = False\n",
        "\n",
        "        # 2. å†è¿›è¡Œå›¾æ–‡ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰\n",
        "        with torch.no_grad():\n",
        "            # ç¡®ä¿æ¨¡å‹åœ¨evalæ¨¡å¼\n",
        "            mobilenet.eval()\n",
        "            contrastive_model.eval()\n",
        "\n",
        "            # æ–‡æœ¬åµŒå…¥ - ä½¿ç”¨ no_grad\n",
        "            text_emb = embedder.encode(report_text, convert_to_tensor=True, device=device).unsqueeze(0)\n",
        "\n",
        "            # å…³é”®ä¿®å¤ï¼šç¡®ä¿æ–‡æœ¬åµŒå…¥ä¸åœ¨æ¨ç†æ¨¡å¼ä¸‹\n",
        "            if hasattr(text_emb, 'is_inference') and text_emb.is_inference:\n",
        "                text_emb = text_emb.clone().requires_grad_(False)\n",
        "\n",
        "            # å›¾åƒç‰¹å¾æå–\n",
        "            preprocess = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "            ])\n",
        "            img_tensor = preprocess(image_pil).unsqueeze(0).to(device)\n",
        "\n",
        "            # è·å–å›¾åƒç‰¹å¾\n",
        "            img_feat = mobilenet.features(img_tensor)\n",
        "            img_feat = F.adaptive_avg_pool2d(img_feat, (1, 1)).view(1, -1)\n",
        "\n",
        "            # æŠ•å½±åˆ°å¯¹æ¯”ç©ºé—´\n",
        "            try:\n",
        "                img_proj = contrastive_model(img=img_feat)\n",
        "                text_proj = contrastive_model(text=text_emb)\n",
        "                similarity = F.cosine_similarity(img_proj, text_proj).item()\n",
        "                similarity = max(0.0, min(1.0, (similarity + 1) / 2))  # å½’ä¸€åŒ–åˆ°0-1\n",
        "            except Exception as e:\n",
        "                st.error(f\"å›¾æ–‡åŒ¹é…è®¡ç®—å¤±è´¥: {e}\")\n",
        "                similarity = 0.5\n",
        "\n",
        "            # å…³é”®è¯é«˜äº®\n",
        "            keywords = extract_keywords(report_text, top_n=15)\n",
        "            highlighted_report = report_text\n",
        "            for kw in keywords:\n",
        "                if len(kw) > 1 and kw in highlighted_report:\n",
        "                    highlighted_report = highlighted_report.replace(\n",
        "                        kw, f'<mark style=\"background-color:#FFF59D;padding:3px;border-radius:3px\">{kw}</mark>'\n",
        "                    )\n",
        "\n",
        "    # æ˜¾ç¤ºç»“æœ\n",
        "    st.success(\"åˆ†æå®Œæˆï¼\")\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "    with col1:\n",
        "        st.image(image_pil.convert('RGB'), caption=\"åŸå§‹Xå…‰å›¾åƒ\", width='stretch')\n",
        "        st.image(overlay_pil, caption=\"çº¢è‰²åŒºåŸŸè¡¨ç¤ºæ¨¡å‹å…³æ³¨çš„å¯ç–‘è‚ºç‚åŒºåŸŸ\", width='stretch')\n",
        "\n",
        "    with col2:\n",
        "        st.markdown(\"### æ”¾å°„ç§‘æŠ¥å‘Š\")\n",
        "        st.markdown(highlighted_report, unsafe_allow_html=True)\n",
        "\n",
        "        st.markdown(\"### ğŸ§  AI å¤šæ¨¡æ€è¾…åŠ©åˆ†æç»“è®º\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = mobilenet(img_tensor)\n",
        "            prob = F.softmax(output, dim=1)[0]\n",
        "            pneumonia_prob = prob[1].item()\n",
        "\n",
        "        if pneumonia_prob > 0.7:\n",
        "            diagnosis = \"è‚ºç‚ç–‘ä¼¼é˜³æ€§\"; color = \"ğŸ”´\"; st.info(\"ğŸ“Š **è‚ºç‚æ¦‚ç‡è¾ƒé«˜ï¼Œå»ºè®®ä¸´åºŠé‡ç‚¹å…³æ³¨**\")\n",
        "        elif pneumonia_prob < 0.4:\n",
        "            diagnosis = \"è‚ºç‚ç–‘ä¼¼é˜´æ€§\"; color = \"ğŸŸ¢\"; st.info(\"ğŸ“Š **å€¾å‘äºæ­£å¸¸è¡¨ç°**\")\n",
        "        else:\n",
        "            diagnosis = \"è‚ºç‚å¯èƒ½æ€§ä¸ç¡®å®š\"; color = \"ğŸŸ¡\"; st.info(\"ğŸ“Š **éœ€ç»“åˆä¸´åºŠè¿›ä¸€æ­¥åˆ¤æ–­**\")\n",
        "\n",
        "        st.markdown(f\"**{color} AI è¾…åŠ©å»ºè®®ï¼š{diagnosis}**\")\n",
        "        st.metric(label=\"å›¾åƒåˆ†ç±»ç½®ä¿¡åº¦ï¼ˆè‚ºç‚æ¦‚ç‡ï¼‰\", value=f\"{pneumonia_prob:.1%}\")\n",
        "\n",
        "        with st.expander(\"å›¾æ–‡å¤šæ¨¡æ€ä¸€è‡´æ€§è¯„ä¼°\"):\n",
        "            st.markdown(\"ç³»ç»Ÿè¯„ä¼°å›¾åƒä¸æŠ¥å‘Šæè¿°çš„è¯­ä¹‰ä¸€è‡´æ€§ï¼ˆå¼€å‘æŒç»­ä¼˜åŒ–ä¸­ï¼‰\")\n",
        "            level = \"è¾ƒé«˜\" if similarity > 0.7 else \"ä¸­ç­‰\" if similarity > 0.5 else \"è¾ƒä½\"\n",
        "            st.metric(\"å›¾æ–‡è¯­ä¹‰ä¸€è‡´æ€§åˆ†æ•°\", f\"{similarity:.4f}\")\n",
        "            st.write(f\"ä¸€è‡´æ€§æ°´å¹³ï¼š**{level}**\")\n",
        "\n",
        "        st.caption(\"æœ¬ç³»ç»Ÿä»…ä¾›ä¸´åºŠè¾…åŠ©å‚è€ƒï¼Œä¸æ›¿ä»£åŒ»ç”Ÿè¯Šæ–­\")\n",
        "\n",
        "else:\n",
        "    if uploaded_image is None:\n",
        "        st.warning(\"è¯·ä¸Šä¼ èƒ¸éƒ¨Xå…‰å›¾åƒ\")\n",
        "    if report_text.strip() == \"\":\n",
        "        st.warning(\"è¯·ä¸Šä¼ æˆ–ç²˜è´´æ”¾å°„ç§‘æŠ¥å‘Šæ–‡æœ¬\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwPp-KIG1vqa",
        "outputId": "d6f16d80-60b7-4254-fb42-56ae340a5916"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    }
  ]
}