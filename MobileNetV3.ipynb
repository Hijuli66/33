{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hijuli66/33/blob/master/MobileNetV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOLxYZiNK7RF",
        "outputId": "c1e39de5-3776-41e3-95fb-999b4d71d19f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Google Drive 已挂载\n",
            "成功加载 train.csv, val.csv 和 test.csv\n",
            "训练集大小：7681，验证集大小：961，测试集大小：961\n",
            "训练前初始验证：评估预训练模型在验证集上的性能\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "初始验证集 损失：0.6818，准确率：0.5744，F1 分数：0.5736\n",
            "开始阶段 1：完整训练（只训练顶层）\n",
            "阶段 1 Epoch 1/10 验证集 损失：0.5708，准确率：0.6785，F1 分数：0.6532\n",
            "阶段 1 Epoch 2/10 验证集 损失：0.3345，准确率：0.8491，F1 分数：0.8461\n",
            "阶段 1 Epoch 3/10 验证集 损失：0.2745，准确率：0.8866，F1 分数：0.8855\n",
            "阶段 1 Epoch 4/10 验证集 损失：0.2487，准确率：0.9074，F1 分数：0.9069\n",
            "阶段 1 Epoch 5/10 验证集 损失：0.2840，准确率：0.8762，F1 分数：0.8740\n",
            "阶段 1 Epoch 6/10 验证集 损失：0.2528，准确率：0.8866，F1 分数：0.8864\n",
            "阶段 1 Epoch 7/10 验证集 损失：0.2246，准确率：0.9105，F1 分数：0.9099\n",
            "阶段 1 Epoch 8/10 验证集 损失：0.3257，准确率：0.8491，F1 分数：0.8473\n",
            "阶段 1 Epoch 9/10 验证集 损失：0.2377，准确率：0.8980，F1 分数：0.8970\n",
            "阶段 1 Epoch 10/10 验证集 损失：0.2604，准确率：0.8855，F1 分数：0.8853\n",
            "开始阶段 2：完整训练（解冻后几层）\n",
            "阶段 2 Epoch 1/10 验证集 损失：0.1690，准确率：0.9334，F1 分数：0.9333\n",
            "阶段 2 Epoch 2/10 验证集 损失：0.1461，准确率：0.9428，F1 分数：0.9427\n",
            "阶段 2 Epoch 3/10 验证集 损失：0.1370，准确率：0.9469，F1 分数：0.9469\n",
            "阶段 2 Epoch 4/10 验证集 损失：0.1360，准确率：0.9459，F1 分数：0.9459\n",
            "阶段 2 Epoch 5/10 验证集 损失：0.1251，准确率：0.9480，F1 分数：0.9480\n",
            "阶段 2 Epoch 6/10 验证集 损失：0.1214，准确率：0.9501，F1 分数：0.9500\n",
            "阶段 2 Epoch 7/10 验证集 损失：0.1197，准确率：0.9542，F1 分数：0.9542\n",
            "阶段 2 Epoch 8/10 验证集 损失：0.1217，准确率：0.9490，F1 分数：0.9490\n",
            "阶段 2 Epoch 9/10 验证集 损失：0.1182，准确率：0.9490，F1 分数：0.9490\n",
            "阶段 2 Epoch 10/10 验证集 损失：0.1176，准确率：0.9480，F1 分数：0.9480\n",
            "最终模型已保存至：/content/drive/MyDrive/33/Images/Models//mobilenetv3_final.pth\n",
            "开始测试集评估\n",
            "测试集 损失：0.1260，准确率：0.9646，F1 分数：0.9646\n",
            "测试集预测结果已保存至：/content/drive/MyDrive/33/Data//test_predictions.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "\n",
        "# 挂载 Google Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "else:\n",
        "    print(\"Google Drive 已挂载\")\n",
        "\n",
        "# 设置路径\n",
        "drive_path = '/content/drive/MyDrive/33/'\n",
        "data_path = os.path.join(drive_path, 'Data/')\n",
        "model_path = os.path.join(drive_path, 'Images/Models/')\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "# 1. 加载和准备数据\n",
        "try:\n",
        "    train_df = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
        "    val_df = pd.read_csv(os.path.join(data_path, 'val.csv'))\n",
        "    test_df = pd.read_csv(os.path.join(data_path, 'test.csv'))\n",
        "    print(\"成功加载 train.csv, val.csv 和 test.csv\")\n",
        "    print(f\"训练集大小：{len(train_df)}，验证集大小：{len(val_df)}，测试集大小：{len(test_df)}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"错误：未找到数据集文件，{e}\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"读取数据集时出错：{e}\")\n",
        "    raise\n",
        "\n",
        "# 检查必要列\n",
        "required_columns = ['image_id', 'image_path', 'label']\n",
        "if not all(col in train_df.columns for col in required_columns):\n",
        "    missing_cols = [col for col in required_columns if col not in train_df.columns]\n",
        "    print(f\"错误：train.csv 缺少以下必要列：{missing_cols}\")\n",
        "    raise ValueError(\"数据集缺少必要列\")\n",
        "\n",
        "# 2. 数据增强\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # MobileNetV3 输入尺寸\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.GaussianBlur(kernel_size=3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485], std=[0.229])  # 单通道标准化\n",
        "])\n",
        "\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "])\n",
        "\n",
        "# 自定义数据集类\n",
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(drive_path, self.dataframe.iloc[idx]['image_path'])\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('L')  # 灰度图\n",
        "        except Exception as e:\n",
        "            print(f\"无法加载图像 {img_path}：{e}\")\n",
        "            raise\n",
        "        label = self.dataframe.iloc[idx]['label']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# 创建数据加载器\n",
        "train_dataset = ChestXRayDataset(train_df, transform=train_transforms)\n",
        "val_dataset = ChestXRayDataset(val_df, transform=val_test_transforms)\n",
        "test_dataset = ChestXRayDataset(test_df, transform=val_test_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# 3. 加载 MobileNetV3 并修改输入层\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.mobilenet_v3_small(pretrained=True)\n",
        "model.features[0][0] = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)  # 单通道输入\n",
        "num_features = model.classifier[3].in_features\n",
        "model.classifier[3] = nn.Linear(num_features, 2)  # 二分类\n",
        "model = model.to(device)\n",
        "\n",
        "# 评估函数（返回损失、准确率、F1 分数和预测）\n",
        "def evaluate_model(model, data_loader, criterion, dataset_name=\"验证集\"):\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels_batch in data_loader:\n",
        "            inputs, labels_batch = inputs.to(device), labels_batch.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels_batch)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            preds.extend(pred.cpu().numpy())\n",
        "            labels.extend(labels_batch.cpu().numpy())\n",
        "    loss = running_loss / len(data_loader.dataset)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    print(f\"{dataset_name} 损失：{loss:.4f}，准确率：{acc:.4f}，F1 分数：{f1:.4f}\")\n",
        "    return loss, acc, f1, preds\n",
        "\n",
        "# 训练函数\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, phase, model_save_path):\n",
        "    best_val_f1 = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        # 训练阶段\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "        # 验证阶段（在训练后）\n",
        "        val_loss, val_acc, val_f1, _ = evaluate_model(model, val_loader, criterion, dataset_name=f\"阶段 {phase} Epoch {epoch+1}/{num_epochs} 验证集\")\n",
        "\n",
        "        # 保存最佳模型\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "# 4. 初始验证（训练前）\n",
        "print(\"训练前初始验证：评估预训练模型在验证集上的性能\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "evaluate_model(model, val_loader, criterion, dataset_name=\"初始验证集\")\n",
        "\n",
        "# 5. 阶段 1：完整训练（只训练顶层）\n",
        "print(\"开始阶段 1：完整训练（只训练顶层）\")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, phase=1, model_save_path=os.path.join(model_path, 'mobilenetv3_full_phase1.pth'))\n",
        "\n",
        "# 6. 阶段 2：完整训练（解冻后几层）\n",
        "print(\"开始阶段 2：完整训练（解冻后几层）\")\n",
        "for name, param in model.named_parameters():\n",
        "    if \"features.9\" in name or \"features.10\" in name or \"features.11\" in name:\n",
        "        param.requires_grad = True\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001, weight_decay=0.0001)\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, phase=2, model_save_path=os.path.join(model_path, 'mobilenetv3_full_phase2.pth'))\n",
        "\n",
        "# 保存最终模型\n",
        "torch.save(model.state_dict(), os.path.join(model_path, 'mobilenetv3_final.pth'))\n",
        "print(f\"最终模型已保存至：{model_path}/mobilenetv3_final.pth\")\n",
        "\n",
        "# 7. 测试集评估\n",
        "print(\"开始测试集评估\")\n",
        "model.load_state_dict(torch.load(os.path.join(model_path, 'mobilenetv3_final.pth')))\n",
        "test_loss, test_acc, test_f1, test_preds = evaluate_model(model, test_loader, criterion, dataset_name=\"测试集\")\n",
        "\n",
        "# 保存测试集预测结果\n",
        "test_df['prediction'] = test_preds\n",
        "test_df.to_csv(os.path.join(data_path, 'test_predictions.csv'), index=False)\n",
        "print(f\"测试集预测结果已保存至：{data_path}/test_predictions.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhGjbXPc2taN",
        "outputId": "6280a7c3-ab43-4e0e-9a92-3910c9e819e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Google Drive 已挂载\n",
            "成功加载 test.csv，测试集大小：961\n",
            "特征向量已保存至：/content/drive/MyDrive/33/Images/Models/features.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# 挂载 Google Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "else:\n",
        "    print(\"Google Drive 已挂载\")\n",
        "\n",
        "# 设置路径\n",
        "drive_path = '/content/drive/MyDrive/33/'\n",
        "data_path = os.path.join(drive_path, 'Data/')\n",
        "model_path = os.path.join(drive_path, 'Images/Models/')\n",
        "feature_save_path = os.path.join(model_path, 'features.csv')\n",
        "\n",
        "# 自定义数据集类\n",
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(drive_path, self.dataframe.iloc[idx]['image_path'])\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('L')  # 灰度图\n",
        "        except Exception as e:\n",
        "            print(f\"无法加载图像 {img_path}：{e}\")\n",
        "            raise\n",
        "        label = self.dataframe.iloc[idx]['label']\n",
        "        image_id = self.dataframe.iloc[idx]['image_id']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label, image_id, img_path\n",
        "\n",
        "# 数据变换（与训练一致）\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "])\n",
        "\n",
        "# 加载测试集\n",
        "try:\n",
        "    test_df = pd.read_csv(os.path.join(data_path, 'test.csv'))\n",
        "    print(f\"成功加载 test.csv，测试集大小：{len(test_df)}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"错误：未找到 test.csv，{e}\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"读取 test.csv 时出错：{e}\")\n",
        "    raise\n",
        "\n",
        "# 检查必要列\n",
        "required_columns = ['image_id', 'image_path', 'label']\n",
        "if not all(col in test_df.columns for col in required_columns):\n",
        "    missing_cols = [col for col in required_columns if col not in test_df.columns]\n",
        "    print(f\"错误：test.csv 缺少以下必要列：{missing_cols}\")\n",
        "    raise ValueError(\"数据集缺少必要列\")\n",
        "\n",
        "# 创建数据加载器\n",
        "test_dataset = ChestXRayDataset(test_df, transform=test_transforms)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# 加载模型\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.mobilenet_v3_small(weights=None)\n",
        "model.features[0][0] = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)  # 单通道输入\n",
        "num_features = model.classifier[3].in_features\n",
        "model.classifier[3] = nn.Linear(num_features, 2)  # 二分类\n",
        "try:\n",
        "    model.load_state_dict(torch.load(os.path.join(model_path, 'mobilenetv3_final.pth')))\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"错误：未找到 mobilenetv3_final.pth，{e}\")\n",
        "    raise\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 清理 GPU 内存\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 提取特征函数\n",
        "def extract_features(model, data_loader):\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "    image_ids_list = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, image_ids, _ in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            # 提取 features 层输出（全局平均池化前）\n",
        "            features = model.features(inputs)  # 输出 [batch, 576, 7, 7]\n",
        "            features = torch.nn.functional.adaptive_avg_pool2d(features, (1, 1))  # 全局平均池化 [batch, 576, 1, 1]\n",
        "            features = features.squeeze(-1).squeeze(-1)  # 扁平化为 [batch, 576]\n",
        "            features_list.extend(features.cpu().numpy())\n",
        "            labels_list.extend(labels.numpy())\n",
        "            image_ids_list.extend(image_ids)\n",
        "    return image_ids_list, features_list, labels_list\n",
        "\n",
        "# 提取特征\n",
        "image_ids, features, labels = extract_features(model, test_loader)\n",
        "\n",
        "# 保存为 CSV\n",
        "feature_df = pd.DataFrame({\n",
        "    'image_id': image_ids,\n",
        "    'feature_vector': [','.join(map(str, f)) for f in features],  # 转换为逗号分隔字符串\n",
        "    'label': labels\n",
        "})\n",
        "feature_df.to_csv(feature_save_path, index=False)\n",
        "print(f\"特征向量已保存至：{feature_save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su3LHkzzf4eu"
      },
      "source": [
        "下面是热力图代码"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6yURM6suKOp"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchxrayvision\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torchxrayvision as xrv\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ====================== 保存路径 ======================\n",
        "drive_path = '/content/drive/MyDrive/33/'\n",
        "control_dir = os.path.join(drive_path, 'Images/QaTa-dataset/control_images')\n",
        "covid_dir   = os.path.join(drive_path, 'Images/QaTa-dataset/QaTa-COV19')\n",
        "result_dir  = os.path.join(drive_path, 'Images/Models/')   # ← 你要的路径\n",
        "os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "# ====================== 加载模型 ======================\n",
        "print(\"正在加载 TorchXRayVision PSPNet 模型（肺部分割）...\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = xrv.baseline_models.chestx_det.PSPNet()\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(\"模型加载完成！\")\n",
        "\n",
        "# ====================== 关键函数：自动中心裁剪成正方形 ======================\n",
        "def center_crop_to_square(img_np):\n",
        "    h, w = img_np.shape[:2]\n",
        "    min_side = min(h, w)\n",
        "    start_h = (h - min_side) // 2\n",
        "    start_w = (w - min_side) // 2\n",
        "    return img_np[start_h:start_h+min_side, start_w:start_w+min_side]\n",
        "\n",
        "# ====================== 处理函数 ======================\n",
        "def process_folder(image_folder, label_name, num_images=10):\n",
        "    print(f\"\\n正在处理：{label_name}（共 {num_images} 张）\")\n",
        "    all_images = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    selected = all_images[:num_images]\n",
        "\n",
        "    for filename in tqdm(selected, desc=label_name):\n",
        "        img_path = os.path.join(image_folder, filename)\n",
        "        img_pil = Image.open(img_path).convert('L')\n",
        "        img_np = np.array(img_pil)\n",
        "\n",
        "        # Step 1: 中心裁剪成正方形\n",
        "        img_square = center_crop_to_square(img_np)\n",
        "\n",
        "        # Step 2: 归一化 + 转为 tensor + resize 到 512×512\n",
        "        img_norm = xrv.datasets.normalize(img_square, 255)\n",
        "        img_tensor = torch.from_numpy(img_norm).float().unsqueeze(0).unsqueeze(0).to(device)  # (1,1,H,W)\n",
        "        img_tensor = torch.nn.functional.interpolate(img_tensor, size=(512, 512), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Step 3: 推理\n",
        "        with torch.no_grad():\n",
        "            output = model(img_tensor)\n",
        "            prob = torch.softmax(output, dim=1)\n",
        "        lung_prob = torch.max(prob[0, [4, 5]], dim=0)[0].cpu().numpy()  # 左右肺取最大\n",
        "        mask_512 = (lung_prob > 0.5).astype(np.uint8) * 255\n",
        "\n",
        "        # Step 4: 统一输出到 224×224\n",
        "        img_224 = cv2.resize(img_square, (224, 224))\n",
        "        mask_224 = cv2.resize(mask_512, (224, 224), interpolation=cv2.INTER_NEAREST)\n",
        "        mask_224 = cv2.dilate(mask_224, np.ones((3,3), np.uint8), iterations=1)\n",
        "\n",
        "        # Step 5: 绿色叠加\n",
        "        overlay = cv2.cvtColor(img_224, cv2.COLOR_GRAY2BGR)\n",
        "        overlay[mask_224 > 127] = [0, 255, 0]\n",
        "        blended = cv2.addWeighted(overlay, 0.4, cv2.cvtColor(img_224, cv2.COLOR_GRAY2BGR), 0.6, 0)\n",
        "\n",
        "        # Step 6: 三联图保存\n",
        "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "        axs[0].imshow(img_224, cmap='gray'); axs[0].set_title('原图 Original'); axs[0].axis('off')\n",
        "        axs[1].imshow(mask_224, cmap='gray'); axs[1].set_title('肺部掩码 Lung Mask'); axs[1].axis('off')\n",
        "        axs[2].imshow(cv2.cvtColor(blended, cv2.COLOR_BGR2RGB)); axs[2].set_title('绿色叠加 Overlay'); axs[2].axis('off')\n",
        "        plt.suptitle(f\"{filename} - {label_name.upper()}\", fontsize=16)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        save_name = os.path.splitext(filename)[0] + f\"_{label_name}_comparison.png\"\n",
        "        plt.savefig(os.path.join(result_dir, save_name), dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "# ====================== 开始运行 ======================\n",
        "process_folder(control_dir, \"control\", num_images=10)\n",
        "process_folder(covid_dir,   \"covid\",   num_images=10)\n",
        "\n",
        "print(f\"\\n全部完成！20 张高清对比图已保存到：\")\n",
        "print(result_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Yf-S7pPWLUV",
        "outputId": "bcbb1f24-4ddc-43dc-b854-8890d52f0ec2"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "热力图将保存到：\n",
            "  正常组 → /content/drive/MyDrive/33/Images/Heatmaps/control\n",
            "  肺炎组 → /content/drive/MyDrive/33/Images/Heatmaps/covid\n",
            "正在加载 MobileNetV3 肺炎分类模型...\n",
            "正在加载 PSPNet 肺部分割模型...\n",
            "\n",
            "开始处理 control 组\n",
            "  总图像：5000 张  |  已完成：5000 张  |  剩余：0 张\n",
            "  control 组已全部完成，无需处理！\n",
            "\n",
            "开始处理 covid 组\n",
            "  总图像：4603 张  |  已完成：3325 张  |  剩余：1278 张\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "covid 处理中: 100%|██████████| 1278/1278 [4:01:05<00:00, 11.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "covid 组全部处理完成！\n",
            "  本次新增：1278 张  |  累计完成：4603/4603 张\n",
            "\n",
            "所有热力图生成任务已完成或已存在！\n",
            "查看路径：/content/drive/MyDrive/33/Images/Heatmaps\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torchxrayvision tqdm\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchxrayvision as xrv\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "\n",
        "# ====================== 挂载 & 路径设置 ======================\n",
        "drive.mount('/content/drive')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "drive_path = '/content/drive/MyDrive/33/'\n",
        "\n",
        "# 数据文件夹\n",
        "data_root = os.path.join(drive_path, 'Images/QaTa-dataset/')\n",
        "control_dir = os.path.join(data_root, 'control_images')\n",
        "covid_dir   = os.path.join(data_root, 'QaTa-COV19')\n",
        "\n",
        "# 模型路径\n",
        "model_path = os.path.join(drive_path, 'Images/Models/mobilenetv3_final.pth')\n",
        "\n",
        "# 最终输出路径（分 normal / covid 两个子文件夹）\n",
        "base_heatmap_dir = os.path.join(drive_path, 'Images/Heatmaps')\n",
        "control_heatmap_dir = os.path.join(base_heatmap_dir, 'control')\n",
        "covid_heatmap_dir   = os.path.join(base_heatmap_dir, 'covid')\n",
        "os.makedirs(control_heatmap_dir, exist_ok=True)\n",
        "os.makedirs(covid_heatmap_dir, exist_ok=True)\n",
        "\n",
        "print(f\"热力图将保存到：\")\n",
        "print(f\"  正常组 → {control_heatmap_dir}\")\n",
        "print(f\"  肺炎组 → {covid_heatmap_dir}\")\n",
        "\n",
        "# ====================== 1. 加载分类模型 ======================\n",
        "print(\"正在加载 MobileNetV3 肺炎分类模型...\")\n",
        "model = models.mobilenet_v3_small(weights=None)\n",
        "model.features[0][0] = torch.nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "num_ftrs = model.classifier[3].in_features\n",
        "model.classifier[3] = torch.nn.Linear(num_ftrs, 2)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "target_layer = model.features[-1]\n",
        "\n",
        "# ====================== 2. 加载肺部分割模型 ======================\n",
        "print(\"正在加载 PSPNet 肺部分割模型...\")\n",
        "seg_model = xrv.baseline_models.chestx_det.PSPNet()\n",
        "seg_model.to(device)\n",
        "seg_model.eval()\n",
        "\n",
        "# ====================== 3. 预处理 ======================\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "])\n",
        "\n",
        "# ====================== Grad-CAM & 掩码函数 ======================\n",
        "def get_gradcam_heatmap(img_tensor, target_class):\n",
        "    model.zero_grad()\n",
        "    gradients, activations = [], []\n",
        "    def save_grad(grad): gradients.append(grad.detach())\n",
        "    def save_act(module, input, output): activations.append(output.detach())\n",
        "    h1 = target_layer.register_forward_hook(save_act)\n",
        "    h2 = target_layer.register_full_backward_hook(lambda m, gi, go: save_grad(go[0]))\n",
        "    pred = model(img_tensor)\n",
        "    score = pred[0, target_class]\n",
        "    score.backward()\n",
        "    grads = gradients[0]\n",
        "    acts = activations[0]\n",
        "    h1.remove(); h2.remove()\n",
        "    weights = torch.mean(grads, dim=(2,3), keepdim=True)\n",
        "    cam = torch.sum(weights * acts, dim=1).squeeze(0)\n",
        "    cam = F.relu(cam).cpu().numpy()\n",
        "    cam = cv2.resize(cam, (224, 224))\n",
        "    cam = np.maximum(cam, 0)\n",
        "    if cam.max() > 0:\n",
        "        cam = cam / cam.max()\n",
        "    return cam\n",
        "\n",
        "def get_lung_mask(img_pil):\n",
        "    img_np = np.array(img_pil.convert('L'))\n",
        "    img_norm = xrv.datasets.normalize(img_np, 255)\n",
        "    img_tensor = torch.from_numpy(img_norm).float().unsqueeze(0).unsqueeze(0).to(device)\n",
        "    img_tensor = F.interpolate(img_tensor, size=(512,512), mode='bilinear', align_corners=False)\n",
        "    with torch.no_grad():\n",
        "        output = seg_model(img_tensor)\n",
        "        prob = torch.softmax(output, dim=1)\n",
        "        lung_prob = torch.max(prob[0, [4,5]], dim=0)[0]\n",
        "        mask = (lung_prob > 0.5).float().cpu().numpy()\n",
        "        mask = cv2.resize(mask, (224,224), interpolation=cv2.INTER_NEAREST)\n",
        "    return (mask > 0.5).astype(np.uint8)\n",
        "\n",
        "# ====================== 断点续传主函数=====================\n",
        "def process_folder_with_resume(folder_path, save_dir, label_name, class_idx):\n",
        "    all_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    # 关键：获取已生成的热力图文件\n",
        "    existing_files = set()\n",
        "    if os.path.exists(save_dir):\n",
        "        for f in os.listdir(save_dir):\n",
        "            if f.startswith(label_name + \"_\"):\n",
        "                # 提取原始文件名：control_xxx.png → xxx.png\n",
        "                orig_name = f[len(label_name)+1:]  # 去掉 \"control_\" 或 \"covid_\"\n",
        "                existing_files.add(orig_name)\n",
        "\n",
        "    # 过滤掉已经处理过的\n",
        "    todo_files = [f for f in all_files if f not in existing_files]\n",
        "\n",
        "    total = len(all_files)\n",
        "    done = len(existing_files)\n",
        "    remain = len(todo_files)\n",
        "\n",
        "    print(f\"\\n开始处理 {label_name} 组\")\n",
        "    print(f\"  总图像：{total} 张  |  已完成：{done} 张  |  剩余：{remain} 张\")\n",
        "\n",
        "    if remain == 0:\n",
        "        print(f\"  {label_name} 组已全部完成，无需处理！\")\n",
        "        return\n",
        "\n",
        "    for filename in tqdm(todo_files, desc=f\"{label_name} 处理中\"):\n",
        "        img_path = os.path.join(folder_path, filename)\n",
        "        try:\n",
        "            img_pil = Image.open(img_path).convert('L')\n",
        "        except Exception as e:\n",
        "            print(f\"\\n跳过损坏图像：{filename} ({e})\")\n",
        "            continue\n",
        "\n",
        "        img_tensor = transform(img_pil).unsqueeze(0).to(device)\n",
        "        cam = get_gradcam_heatmap(img_tensor, class_idx)\n",
        "        lung_mask = get_lung_mask(img_pil)\n",
        "        cam_masked = cam * lung_mask\n",
        "\n",
        "        img_np = np.array(img_pil)\n",
        "        img_224 = cv2.resize(img_np, (224, 224))\n",
        "        heatmap_color = cv2.applyColorMap(np.uint8(255 * cam_masked), cv2.COLORMAP_JET)\n",
        "        superimposed = cv2.addWeighted(cv2.cvtColor(img_224, cv2.COLOR_GRAY2BGR), 0.6, heatmap_color, 0.4, 0)\n",
        "\n",
        "        # 四联图\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
        "        titles = ['原图', '肺部掩码', 'Grad-CAM（仅肺区）', '最终热力图']\n",
        "        imgs = [img_224, lung_mask * 255, cam_masked, cv2.cvtColor(superimposed, cv2.COLOR_BGR2RGB)]\n",
        "        cmaps = ['gray', 'gray', 'jet', None]\n",
        "\n",
        "        for i in range(4):\n",
        "            if cmaps[i] == 'jet':\n",
        "                im = axs[i].imshow(imgs[i], cmap='jet', vmin=0, vmax=1)\n",
        "                plt.colorbar(im, ax=axs[i], fraction=0.046, pad=0.04)\n",
        "            else:\n",
        "                axs[i].imshow(imgs[i], cmap=cmaps[i])\n",
        "            axs[i].set_title(titles[i])\n",
        "            axs[i].axis('off')\n",
        "\n",
        "        plt.suptitle(f\"{filename} - {label_name.upper()}\", fontsize=16)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        save_name = f\"{label_name}_{filename}\"\n",
        "        plt.savefig(os.path.join(save_dir, save_name), dpi=200, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "    print(f\"{label_name} 组全部处理完成！\")\n",
        "    print(f\"  本次新增：{remain} 张  |  累计完成：{done + remain}/{total} 张\")\n",
        "\n",
        "# ====================== 运行（支持断点续传）=====================\n",
        "process_folder_with_resume(control_dir, control_heatmap_dir, \"control\", class_idx=0)\n",
        "process_folder_with_resume(covid_dir,   covid_heatmap_dir,   \"covid\",   class_idx=1)\n",
        "\n",
        "print(\"\\n所有热力图生成任务已完成或已存在！\")\n",
        "print(f\"查看路径：{base_heatmap_dir}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}