{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hijuli66/33/blob/master/MobileNetV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "\n",
        "# 挂载 Google Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "else:\n",
        "    print(\"Google Drive 已挂载\")\n",
        "\n",
        "# 设置路径\n",
        "drive_path = '/content/drive/MyDrive/33/'\n",
        "data_path = os.path.join(drive_path, 'Data/')\n",
        "model_path = os.path.join(drive_path, 'Images/Models/')\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "# 1. 加载和准备数据\n",
        "try:\n",
        "    train_df = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
        "    val_df = pd.read_csv(os.path.join(data_path, 'val.csv'))\n",
        "    test_df = pd.read_csv(os.path.join(data_path, 'test.csv'))\n",
        "    print(\"成功加载 train.csv, val.csv 和 test.csv\")\n",
        "    print(f\"训练集大小：{len(train_df)}，验证集大小：{len(val_df)}，测试集大小：{len(test_df)}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"错误：未找到数据集文件，{e}\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"读取数据集时出错：{e}\")\n",
        "    raise\n",
        "\n",
        "# 检查必要列\n",
        "required_columns = ['image_id', 'image_path', 'label']\n",
        "if not all(col in train_df.columns for col in required_columns):\n",
        "    missing_cols = [col for col in required_columns if col not in train_df.columns]\n",
        "    print(f\"错误：train.csv 缺少以下必要列：{missing_cols}\")\n",
        "    raise ValueError(\"数据集缺少必要列\")\n",
        "\n",
        "# 2. 数据增强\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # MobileNetV3 输入尺寸\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.GaussianBlur(kernel_size=3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485], std=[0.229])  # 单通道标准化\n",
        "])\n",
        "\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "])\n",
        "\n",
        "# 自定义数据集类\n",
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(drive_path, self.dataframe.iloc[idx]['image_path'])\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('L')  # 灰度图\n",
        "        except Exception as e:\n",
        "            print(f\"无法加载图像 {img_path}：{e}\")\n",
        "            raise\n",
        "        label = self.dataframe.iloc[idx]['label']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# 创建数据加载器\n",
        "train_dataset = ChestXRayDataset(train_df, transform=train_transforms)\n",
        "val_dataset = ChestXRayDataset(val_df, transform=val_test_transforms)\n",
        "test_dataset = ChestXRayDataset(test_df, transform=val_test_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# 3. 加载 MobileNetV3 并修改输入层\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.mobilenet_v3_small(pretrained=True)\n",
        "model.features[0][0] = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)  # 单通道输入\n",
        "num_features = model.classifier[3].in_features\n",
        "model.classifier[3] = nn.Linear(num_features, 2)  # 二分类\n",
        "model = model.to(device)\n",
        "\n",
        "# 评估函数（返回损失、准确率、F1 分数和预测）\n",
        "def evaluate_model(model, data_loader, criterion, dataset_name=\"验证集\"):\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels_batch in data_loader:\n",
        "            inputs, labels_batch = inputs.to(device), labels_batch.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels_batch)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "            preds.extend(pred.cpu().numpy())\n",
        "            labels.extend(labels_batch.cpu().numpy())\n",
        "    loss = running_loss / len(data_loader.dataset)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    print(f\"{dataset_name} 损失：{loss:.4f}，准确率：{acc:.4f}，F1 分数：{f1:.4f}\")\n",
        "    return loss, acc, f1, preds\n",
        "\n",
        "# 训练函数\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, phase, model_save_path):\n",
        "    best_val_f1 = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        # 训练阶段\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "        # 验证阶段（在训练后）\n",
        "        val_loss, val_acc, val_f1, _ = evaluate_model(model, val_loader, criterion, dataset_name=f\"阶段 {phase} Epoch {epoch+1}/{num_epochs} 验证集\")\n",
        "\n",
        "        # 保存最佳模型\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "# 4. 初始验证（训练前）\n",
        "print(\"训练前初始验证：评估预训练模型在验证集上的性能\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "evaluate_model(model, val_loader, criterion, dataset_name=\"初始验证集\")\n",
        "\n",
        "# 5. 阶段 1：完整训练（只训练顶层）\n",
        "print(\"开始阶段 1：完整训练（只训练顶层）\")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, phase=1, model_save_path=os.path.join(model_path, 'mobilenetv3_full_phase1.pth'))\n",
        "\n",
        "# 6. 阶段 2：完整训练（解冻后几层）\n",
        "print(\"开始阶段 2：完整训练（解冻后几层）\")\n",
        "for name, param in model.named_parameters():\n",
        "    if \"features.9\" in name or \"features.10\" in name or \"features.11\" in name:\n",
        "        param.requires_grad = True\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001, weight_decay=0.0001)\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, phase=2, model_save_path=os.path.join(model_path, 'mobilenetv3_full_phase2.pth'))\n",
        "\n",
        "# 保存最终模型\n",
        "torch.save(model.state_dict(), os.path.join(model_path, 'mobilenetv3_final.pth'))\n",
        "print(f\"最终模型已保存至：{model_path}/mobilenetv3_final.pth\")\n",
        "\n",
        "# 7. 测试集评估\n",
        "print(\"开始测试集评估\")\n",
        "model.load_state_dict(torch.load(os.path.join(model_path, 'mobilenetv3_final.pth')))\n",
        "test_loss, test_acc, test_f1, test_preds = evaluate_model(model, test_loader, criterion, dataset_name=\"测试集\")\n",
        "\n",
        "# 保存测试集预测结果\n",
        "test_df['prediction'] = test_preds\n",
        "test_df.to_csv(os.path.join(data_path, 'test_predictions.csv'), index=False)\n",
        "print(f\"测试集预测结果已保存至：{data_path}/test_predictions.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOLxYZiNK7RF",
        "outputId": "c1e39de5-3776-41e3-95fb-999b4d71d19f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive 已挂载\n",
            "成功加载 train.csv, val.csv 和 test.csv\n",
            "训练集大小：7681，验证集大小：961，测试集大小：961\n",
            "训练前初始验证：评估预训练模型在验证集上的性能\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "初始验证集 损失：0.6818，准确率：0.5744，F1 分数：0.5736\n",
            "开始阶段 1：完整训练（只训练顶层）\n",
            "阶段 1 Epoch 1/10 验证集 损失：0.5708，准确率：0.6785，F1 分数：0.6532\n",
            "阶段 1 Epoch 2/10 验证集 损失：0.3345，准确率：0.8491，F1 分数：0.8461\n",
            "阶段 1 Epoch 3/10 验证集 损失：0.2745，准确率：0.8866，F1 分数：0.8855\n",
            "阶段 1 Epoch 4/10 验证集 损失：0.2487，准确率：0.9074，F1 分数：0.9069\n",
            "阶段 1 Epoch 5/10 验证集 损失：0.2840，准确率：0.8762，F1 分数：0.8740\n",
            "阶段 1 Epoch 6/10 验证集 损失：0.2528，准确率：0.8866，F1 分数：0.8864\n",
            "阶段 1 Epoch 7/10 验证集 损失：0.2246，准确率：0.9105，F1 分数：0.9099\n",
            "阶段 1 Epoch 8/10 验证集 损失：0.3257，准确率：0.8491，F1 分数：0.8473\n",
            "阶段 1 Epoch 9/10 验证集 损失：0.2377，准确率：0.8980，F1 分数：0.8970\n",
            "阶段 1 Epoch 10/10 验证集 损失：0.2604，准确率：0.8855，F1 分数：0.8853\n",
            "开始阶段 2：完整训练（解冻后几层）\n",
            "阶段 2 Epoch 1/10 验证集 损失：0.1690，准确率：0.9334，F1 分数：0.9333\n",
            "阶段 2 Epoch 2/10 验证集 损失：0.1461，准确率：0.9428，F1 分数：0.9427\n",
            "阶段 2 Epoch 3/10 验证集 损失：0.1370，准确率：0.9469，F1 分数：0.9469\n",
            "阶段 2 Epoch 4/10 验证集 损失：0.1360，准确率：0.9459，F1 分数：0.9459\n",
            "阶段 2 Epoch 5/10 验证集 损失：0.1251，准确率：0.9480，F1 分数：0.9480\n",
            "阶段 2 Epoch 6/10 验证集 损失：0.1214，准确率：0.9501，F1 分数：0.9500\n",
            "阶段 2 Epoch 7/10 验证集 损失：0.1197，准确率：0.9542，F1 分数：0.9542\n",
            "阶段 2 Epoch 8/10 验证集 损失：0.1217，准确率：0.9490，F1 分数：0.9490\n",
            "阶段 2 Epoch 9/10 验证集 损失：0.1182，准确率：0.9490，F1 分数：0.9490\n",
            "阶段 2 Epoch 10/10 验证集 损失：0.1176，准确率：0.9480，F1 分数：0.9480\n",
            "最终模型已保存至：/content/drive/MyDrive/33/Images/Models//mobilenetv3_final.pth\n",
            "开始测试集评估\n",
            "测试集 损失：0.1260，准确率：0.9646，F1 分数：0.9646\n",
            "测试集预测结果已保存至：/content/drive/MyDrive/33/Data//test_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn  # 添加缺失的导入\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# 挂载 Google Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "else:\n",
        "    print(\"Google Drive 已挂载\")\n",
        "\n",
        "# 设置路径\n",
        "drive_path = '/content/drive/MyDrive/33/'\n",
        "data_path = os.path.join(drive_path, 'Data/')\n",
        "model_path = os.path.join(drive_path, 'Images/Models/')\n",
        "feature_save_path = os.path.join(model_path, 'features.csv')\n",
        "\n",
        "# 自定义数据集类\n",
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(drive_path, self.dataframe.iloc[idx]['image_path'])\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('L')  # 灰度图\n",
        "        except Exception as e:\n",
        "            print(f\"无法加载图像 {img_path}：{e}\")\n",
        "            raise\n",
        "        label = self.dataframe.iloc[idx]['label']\n",
        "        image_id = self.dataframe.iloc[idx]['image_id']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label, image_id, img_path\n",
        "\n",
        "# 数据变换（与训练一致）\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485], std=[0.229])\n",
        "])\n",
        "\n",
        "# 加载测试集\n",
        "try:\n",
        "    test_df = pd.read_csv(os.path.join(data_path, 'test.csv'))\n",
        "    print(f\"成功加载 test.csv，测试集大小：{len(test_df)}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"错误：未找到 test.csv，{e}\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"读取 test.csv 时出错：{e}\")\n",
        "    raise\n",
        "\n",
        "# 检查必要列\n",
        "required_columns = ['image_id', 'image_path', 'label']\n",
        "if not all(col in test_df.columns for col in required_columns):\n",
        "    missing_cols = [col for col in required_columns if col not in test_df.columns]\n",
        "    print(f\"错误：test.csv 缺少以下必要列：{missing_cols}\")\n",
        "    raise ValueError(\"数据集缺少必要列\")\n",
        "\n",
        "# 创建数据加载器\n",
        "test_dataset = ChestXRayDataset(test_df, transform=test_transforms)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# 加载模型\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.mobilenet_v3_small(weights=None)  # 不使用 pretrained，以加载自定义权重\n",
        "model.features[0][0] = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)  # 单通道输入\n",
        "num_features = model.classifier[3].in_features\n",
        "model.classifier[3] = nn.Linear(num_features, 2)  # 二分类\n",
        "try:\n",
        "    model.load_state_dict(torch.load(os.path.join(model_path, 'mobilenetv3_final.pth')))\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"错误：未找到 mobilenetv3_final.pth，{e}\")\n",
        "    raise\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 清理 GPU 内存\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 提取特征函数\n",
        "def extract_features(model, data_loader):\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "    image_ids_list = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels, image_ids, _ in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            # 提取 features 层输出（全局平均池化前）\n",
        "            features = model.features(inputs)  # 输出 [batch, 576, 7, 7]\n",
        "            features = torch.nn.functional.adaptive_avg_pool2d(features, (1, 1))  # 全局平均池化 [batch, 576, 1, 1]\n",
        "            features = features.squeeze(-1).squeeze(-1)  # 扁平化为 [batch, 576]\n",
        "            features_list.extend(features.cpu().numpy())\n",
        "            labels_list.extend(labels.numpy())\n",
        "            image_ids_list.extend(image_ids)\n",
        "    return image_ids_list, features_list, labels_list\n",
        "\n",
        "# 提取特征\n",
        "image_ids, features, labels = extract_features(model, test_loader)\n",
        "\n",
        "# 保存为 CSV\n",
        "feature_df = pd.DataFrame({\n",
        "    'image_id': image_ids,\n",
        "    'feature_vector': [','.join(map(str, f)) for f in features],  # 转换为逗号分隔字符串\n",
        "    'label': labels\n",
        "})\n",
        "feature_df.to_csv(feature_save_path, index=False)\n",
        "print(f\"特征向量已保存至：{feature_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhGjbXPc2taN",
        "outputId": "6280a7c3-ab43-4e0e-9a92-3910c9e819e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive 已挂载\n",
            "成功加载 test.csv，测试集大小：961\n",
            "特征向量已保存至：/content/drive/MyDrive/33/Images/Models/features.csv\n"
          ]
        }
      ]
    }
  ]
}